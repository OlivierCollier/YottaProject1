{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dried-contamination",
   "metadata": {},
   "source": [
    "# Prédiction de l'appétence à un produit bancaire - Sélection du modèle\n",
    "\n",
    "Dans un premier temps, nous avons étudié les données fournies par la banque et avons construit des features permettant d'en extraire de l'information de la manière la plus susceptible d'être utile à un algorithme d'apprentissage.\n",
    "\n",
    "Ici, nous proposons et justifions un choix d'algorithme particulier, que nous allons entraîner et optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "promotional-convenience",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-87e8a3d9f7ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfrastructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataBuilderFactory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataMerger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "import model_utilities as mod\n",
    "### LOAD FROM OTHER FILES ###\n",
    "mod.settings()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "\n",
    "from src.infrastructure.build_dataset import DataBuilderFactory, DataMerger\n",
    "from src.domain.cleaning import MissingValueTreatment\n",
    "from src.domain.build_features import feature_engineering_transformer\n",
    "\n",
    "###################\n",
    "## AVERTISSEMENT ##\n",
    "###################\n",
    "\n",
    "# Les constantes utilisées dans ce notebook sont hard-codées. Dans les autres fichiers du projet, \n",
    "# elles sont extraites des fichiers config/base.py et config/column_names.py.\n",
    "# Cependant, dans un souci de clarté et de facilité de lecture, nous souhaitons éviter au lecteur \n",
    "# de ce notebook de devoir systématiquement se reporter à leurs contenus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-corpus",
   "metadata": {},
   "source": [
    "## 1) Création des données\n",
    "\n",
    "Deux jeux de données nous ont été fournis : l'un dont chaque ligne correspond à un contact (ou une série de contacts) avec un client, et l'autre apportant des informations contextuelles sur la situation économique du pays et de la période concernés.\n",
    "\n",
    "Tout d'abord, nous préprocessons ces deux jeux de données séparément. En effet, il est plus facile de traiter les données manquantes dans le fichier de données économiques, qui contient des valeurs mensuelles, que dans le fichier de clients, où chaque ligne est associée à une date précise.\n",
    "\n",
    "Ce traitement comprend :\n",
    "- la reconnaissance colonne par colonne des formats de données (entiers, dates, objets...),\n",
    "- la traduction des mots français afin d'uniformiser la langue à travers les données,\n",
    "- la suppression de colonnes inutiles,\n",
    "- la suppression de lignes possédant trop de valeurs manquantes (quand à la fois AGE et JOB_TYPE ne sont pas renseignés),\n",
    "- pour les données économiques, la complétion des données manquantes par interpolation,\n",
    "- pour les données relatives aux clients, la correction d'erreurs évidentes.\n",
    "\n",
    "Ensuite, les données sont agrégées suivant une colonne MOIS/ANNEE qui est crée pendant le traitement et supprimée tout de suite après l'agrégation. \n",
    "\n",
    "Enfin, on sépare la cible (y) du reste des données (X)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENTS_COLUMNS_TO_DROP\n",
    "\n",
    "# Preprocessing client data\n",
    "client_builder = DataBuilderFactory(client_file, \n",
    "                                    config_client_data, \n",
    "                                    ALL_CLIENT_DATA_TRANSLATION)\n",
    "client_data = client_builder.transform('client')\n",
    "\n",
    "# Preprocessing eco data\n",
    "eco_builder = DataBuilderFactory(eco_file, \n",
    "                                 config_eco_data)\n",
    "eco_data = eco_builder.transform('eco')\n",
    "\n",
    "# Merging files to final dataset\n",
    "X, y = DataMerger(client_data, eco_data, MERGER_FIELD).transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-jacksonville",
   "metadata": {},
   "source": [
    "# 2) Création des ensembles d'entraînement et de test\n",
    "\n",
    "Nous divisons maintenant les données aléatoirement en un ensemble d'apprentissage, qui nous servira pour entraîner le modèle, et un ensemble de test, sur lequel nous l'évaluerons. Nous fixons arbitrairement la valeur de la graine du générateur aléatoire afin d'obtenir des résultats reproductibles.\n",
    "\n",
    "Etant donné que les deux classes à prédire sont particulièrement déséquilibrées -- il y a environ 9 fois plus d'échantillons négatifs que de positifs, il est important d'utiliser l'option stratify, qui permet de s'assurer qu'une répartition proche sera observée à la fois dans l'ensemble d'entraînement et dans celui de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting train and test\n",
    "\n",
    "random_state = 21\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    stratify=y, \n",
    "                                                    random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-gregory",
   "metadata": {},
   "source": [
    "## 3) Création du pipeline\n",
    "\n",
    "D'autre part, nous définissons sous forme d'un pipeline scikit-learn l'ensemble des transformations qui doivent être appliquées aux données, puis passées à un classifieur. On distingue trois composantes dans ce pipeline.\n",
    "\n",
    "#### a) Traitement des valeurs manquantes\n",
    "\n",
    "Les tâches suivantes sont effectuées à travers la classe MissingValueTreatment :\n",
    "- complétion de la variable JOB_TYPE par AGE lorsque c'est possible ; on attribue la catégorie \"Student\" aux moins de 25 ans, la catégorie \"Retired\" aux plus de 60 ans, et la catégorie majoritaire aux autres (un choix plus précis ne changerait presque rien au reste de notre méthode),\n",
    "- complétion des valeurs manquantes des autres variables à l'aide de JOB_TYPE (la valeur majoritaire parmi les éléments de la même catégorie professionnelle pour les variables catégoriques, et la médiane pour les variables continues).\n",
    "\n",
    "#### b) Construction des descripteurs\n",
    "\n",
    "Les descripteurs suivants sont calculés par le transformeur feature_engineering_transformer :\n",
    "- une version clippée entre -4000 et 4000 de ACCOUNT_BALANCE, puis réduite par division par 8000,\n",
    "- des versions clippées entre 0 et 15 de NB_CONTACTS_CURRENT_CAMPAIGN et NB_CONTACTS_BEFORE_CAMPAIGN, puis réduites par division par 15,\n",
    "- des encodages one-hot des variables HAS_HOUSING_LOAN, HAS_PERSO_LOAN, HAS_DEFAULT, MARITAL_STATUS et EDUCATION,\n",
    "- l'indicatrice valant 1 quand JOB_TYPE vaut \"Retired\",\n",
    "- l'indicatrice valant 1 quand RESULT_LAST_CAMPAIGN vaut \"Success\",\n",
    "- l'indicatrice valant 1 quand MARITAL_STATUS vaut \"Single\",\n",
    "- les indicatrices valant 1 respectivement quand AGE est supérieur à 25 et 60,\n",
    "- une version normalisée de la variable AGE par StandardScaler,\n",
    "- une version encodée par target encoding de la variable MONTH, elle-même extraite de DATE,\n",
    "- une variable valant 1 si HAS_HOUSING_LOAN et HAS_PERSO_LOAN valent 1, et 0 sinon,\n",
    "- les variables socio-économiques du tableau supplémentaire, avec une normalisation utilisant StandardScaler.\n",
    "\n",
    "#### c) Classification\n",
    "\n",
    "Nous avons choisi d'utiliser un classifieur par forêt aléatoire pour plusieurs raisons :\n",
    "- notre expérimentation préliminaire nous a montré que les résultats étaient nettement supérieurs avec cette méthode : une dizaine de centième pour l'aire sous la courbe précision-rappel (nous justifierons le choix de ce critère plus loin),\n",
    "- la bonne interprétabilité de ce modèle grâce à la Mean Decrease Impurity implémentée dans la méthode feature_importances_ de RandomForestClassifier,\n",
    "- la disponibilité d'outils d'intelligibilité dédiés aux forêts aléatoires, permettant de mieux expliquer notre méthode au client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-injury",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing global pipeline\n",
    "\n",
    "steps = [\n",
    "    ('imputation', MissingValueTreatment()),\n",
    "    ('feature_engineering', feature_engineering_transformer()),\n",
    "    ('rf_clf', RandomForestClassifier())\n",
    "]\n",
    "\n",
    "pipeline = Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-malaysia",
   "metadata": {},
   "source": [
    "## 4) Optimisation du modèle\n",
    "\n",
    "Une fois ce cadre choisi, on peut optimiser les paramètres de notre forêt aléatoire. Pour ce faire, nous avons d'abord essayé chaque paramètre de RandomForestClassifier et déterminé lesquels avaient un vrai impact, puis avec une approche gloutonne, nous avons recherché des zones plus favorables pour chacun de ces paramètres.\n",
    "\n",
    "Nous en avons déduit un ensemble de paramètres que nous souhaitons tester avec une recherche aléatoire par RandomizedSearchCV. Nous devons alors définir un critère de scoring adapté à notre problème. Or, dans cette étude, il s'agit de maximiser la proportion d'appels fructueux et le nombre de clients intéressés qui sont effectivement contactés, c'est-à-dire la précision et le rappel. Nous choisissons donc d'optimiser une fonction de ces deux quantités : l'aire sous la courbe précision-rappel, qui correspond à l'argument scoring='average_precision' dans la fonction RandomizedSearchCV. \n",
    "\n",
    "Enfin, nous avons utilisé le paramètre class_weight pour donner un poids plus important à la classe minoritaire, inversement proportionnellement à sa représentation dans les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-accounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    'rf_clf__n_estimators': [220, 240, 260, 280, 300],\n",
    "    'rf_clf__max_depth': [6, 7, 8, 9, 10],\n",
    "    'rf_clf__min_samples_leaf': [9, 10, 11, 12]\n",
    "}\n",
    "class_weight = {1: 9, 0: 1}\n",
    "random_state = 21\n",
    "\n",
    "clf = RandomizedSearchCV(estimator=pipeline, \n",
    "                         param_distributions=param_distributions,\n",
    "                         scoring='average_precision', \n",
    "                         random_state=random_state, \n",
    "                         cv=5)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-uruguay",
   "metadata": {},
   "source": [
    "## 5) Evaluation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-cornell",
   "metadata": {},
   "outputs": [],
   "source": [
    "penser à utiliser le pickle pour aller plus vite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-silence",
   "metadata": {},
   "source": [
    "Notre modèle final est donc une forêt aléatoire implémentée dans RandomForestClassifier, avec pour arguments (hors arguments par défaut) :\n",
    "- n_estimators = 220,\n",
    "- max_depth = 10,\n",
    "- min_samples_leaf = 10.\n",
    "\n",
    "On trace sa courbe précision-rappel à partir des prédictions de l'algorithme sur les données de test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-export",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "plot_precision_recall_curve(clf.best_estimator_, X_test, y_test)\n",
    "plt.show()\n",
    "## ajouter titre etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-operation",
   "metadata": {},
   "source": [
    "L'aire sous cette courbe est de 0,46. ## à vérifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "courbe precision-recall\n",
    "aire sous la courbe\n",
    "matrice de confusion\n",
    "cumulative_gain / lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-mayor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-amber",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
